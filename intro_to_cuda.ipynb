{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Basics of GPU Programming**\n",
    "\n",
    "This tutorial will walk you through the process of converting CPU-based Python code to GPU-accelerated code using NUMBA and CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    " 1. [CUDA and NUMBA Introduction](#introduction)\n",
    " 2. [Setting Up Your Environment](#setup)\n",
    " 3. [Basic Concepts](#concepts)\n",
    " 4. [Pattern 1: Simple Array Operations](#array-ops)\n",
    " 5. [Pattern 2: Device Functions](#device-functions)\n",
    " 6. [Pattern 3: Memory Management](#memory)\n",
    " 7. [Pattern 4: Kernel Launch Patterns](#kernels)\n",
    " 8. [Best Practices and Common Pitfalls](#best-practices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CUDA and NUMBA Introduction** <a name=\"introduction\"></a>\n",
    "\n",
    "CUDA (Compute Unified Device Architecture) is NVIDIA's parallel computing platform and programming model designed for their GPUs. While GPUs were originally designed for rendering graphics, CUDA allows them to be used for general-purpose computing (GPGPU).\n",
    "\n",
    "##### Key aspects of CUDA:\n",
    "* Allows direct access to GPU's virtual instruction set\n",
    "* Enables parallel processing across thousands of GPU cores\n",
    "* Only works with NVIDIA GPUs\n",
    "* Traditionally requires writing code in C/C++\n",
    "\n",
    "##### Why GPUs are good for parallel computing:\n",
    "* **CPU vs GPU Architecture**:\n",
    "  * CPUs have few cores (2-64) optimized for sequential tasks\n",
    "  * GPUs have thousands of smaller cores optimized for parallel tasks\n",
    "\n",
    "* **Parallel Processing Benefits**:\n",
    "  * GPUs excel at tasks where the same operation is performed on large amounts of data\n",
    "  * Common applications:\n",
    "    * Scientific computing\n",
    "    * Machine learning\n",
    "    * Simulation\n",
    "    * Data processing\n",
    "\n",
    "### **NUMBA**\n",
    "\n",
    "Numba is a Just-In-Time (JIT) compiler that translates Python code into optimized machine code. It's particularly valuable because of the following features:\n",
    "\n",
    "##### 1. Makes GPU Programming Accessible\n",
    "* **Python-First Approach**: Write CUDA code in Python instead of C/C++\n",
    "* **Automatic Memory Management**: No manual memory allocation/deallocation needed\n",
    "* **Simple Decorators**: Use `@cuda.jit` to designate GPU-accelerated functions\n",
    "\n",
    "##### 2. Offers Multiple Compilation Targets\n",
    "* **CPU Optimization**: Use `@jit` for faster CPU execution\n",
    "* **GPU Acceleration**: Use `@cuda.jit` for GPU-based parallel processing\n",
    "* **Parallel CPU**: Use `@vectorize` for CPU-based parallel execution\n",
    "\n",
    "##### 3. Key Features\n",
    "* **NumPy Integration**: \n",
    "  * Seamless compatibility with NumPy arrays\n",
    "  * Direct support for NumPy operations and functions\n",
    "  \n",
    "* **Python Subset Support**: \n",
    "  * Works with core Python features\n",
    "  * Supports essential NumPy functionality\n",
    "  \n",
    "* **Low-Effort Migration**: \n",
    "  * Minimal code changes needed for GPU acceleration\n",
    "  * Keep existing Python code structure\n",
    "  \n",
    "* **Automatic Optimization**: \n",
    "  * Smart optimization of numerical computations\n",
    "  * Efficient loop transformations\n",
    "  * Vectorization of operations where possible    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setting Up Your Environment** <a name=\"setup\"></a>\n",
    " \n",
    "First, let's import the necessary packages and check if CUDA is available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "from numba import cuda, float64, float32, int32\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = cuda.is_available()\n",
    "print(f\"CUDA available: {cuda_available}\")\n",
    "\n",
    "if not cuda_available:\n",
    "    print(\"\\nCUDA is not available on your system. Here's what you can do:\\n\")\n",
    "    print(\"1. Check if you have an NVIDIA GPU:\")\n",
    "    print(\"   - Windows: Task Manager > Performance tab\")\n",
    "    print(\"   - Linux: Run 'nvidia-smi' in terminal\")\n",
    "    print(\"   - Mac: System Report > Hardware > Graphics/Displays\\n\")\n",
    "    \n",
    "    print(\"2. Install NVIDIA drivers:\")\n",
    "    print(\"   - Download from: https://www.nvidia.com/Download/index.aspx\")\n",
    "    print(\"   - Follow installation instructions for your operating system\\n\")\n",
    "    \n",
    "    print(\"3. Install CUDA Toolkit:\")\n",
    "    print(\"   - Download from: https://developer.nvidia.com/cuda-downloads\")\n",
    "    print(\"   - Ensure version compatibility with your Python environment\\n\")\n",
    "    \n",
    "    print(\"4. Install required Python packages:\")\n",
    "    print(\"   pip install numba cuda-python\\n\")\n",
    "    \n",
    "    print(\"5. Common issues:\")\n",
    "    print(\"   - Mac M1/M2 chips: CUDA is not supported\")\n",
    "    print(\"   - Virtual Environments: May need to reinstall packages\")\n",
    "    print(\"   - Linux: May need to set CUDA_HOME environment variable\\n\")\n",
    "    \n",
    "    print(\"6. Verify installation:\")\n",
    "    print(\"   - Run 'nvcc --version' in terminal\")\n",
    "    print(\"   - Check if nvidia-smi shows your GPU\\n\")\n",
    "    \n",
    "    print(\"Until CUDA is available, you can still follow along with the CPU examples in this tutorial.\")\n",
    "    print(\"The GPU code patterns will be educational, but you won't be able to execute them.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed with the rest of the tutorial, but be aware that if CUDA is not available, \n",
    "the GPU-specific code examples will not execute. You can still learn the patterns and concepts, \n",
    "which will be useful once you have CUDA properly set up.\n",
    "\n",
    "If you're using Google Colab as an alternative, you can get access to a GPU by:\n",
    "1. Go to Runtime > Change runtime type\n",
    "2. Select GPU from the Hardware accelerator drop-down menu\n",
    "3. Click Save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Basic Concepts** <a name=\"concepts\"></a>\n",
    "\n",
    "Before diving into code conversion, let's understand some key GPU programming concepts:\n",
    "\n",
    "- **Thread**: Smallest unit of execution on GPU\n",
    "- **Block**: Group of threads that can share memory\n",
    "- **Grid**: Collection of blocks\n",
    "- **Kernel**: Function that runs on the GPU\n",
    "- **Device**: The GPU\n",
    "- **Host**: The CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pattern 1: Simple Array Operations** <a name=\"array-ops\"></a>\n",
    "\n",
    "Let's start with a simple example: squaring each element in an array.\n",
    "<br>\n",
    "### CPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU Result: [ 1.  4.  9. 16. 25.]\n"
     ]
    }
   ],
   "source": [
    "def cpu_square_array(arr):\n",
    "    result = np.zeros_like(arr)\n",
    "    for i in range(len(arr)):\n",
    "        result[i] = arr[i] * arr[i]\n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "data = np.array([1, 2, 3, 4, 5], dtype=np.float64)\n",
    "result_cpu = cpu_square_array(data)\n",
    "print(\"CPU Result:\", result_cpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid configuration: 1 blocks, 256 threads per block\n",
      "GPU Result: [ 1.  4.  9. 16. 25.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# The @cuda.jit decorator tells Numba this function will run on the GPU\n",
    "# This decorator compiles the function into CUDA device code\n",
    "@cuda.jit\n",
    "def gpu_square_array(in_arr, out_arr):\n",
    "    # cuda.grid(1) gives us the absolute thread ID in the 1D grid\n",
    "    # Each thread will handle one element of the array\n",
    "    idx = cuda.grid(1)\n",
    "    \n",
    "    # We must check if the thread's index is within array bounds\n",
    "    # This is crucial because we might launch more threads than array elements\n",
    "    if idx < in_arr.size:\n",
    "        out_arr[idx] = in_arr[idx] * in_arr[idx]\n",
    "\n",
    "# Create input data - must specify dtype for GPU compatibility\n",
    "data = np.array([1, 2, 3, 4, 5], dtype=np.float64)\n",
    "result_gpu = np.zeros_like(data)\n",
    "\n",
    "# Calculate grid and block sizes\n",
    "# threads_per_block: number of threads in each block (hardware limit is typically 1024)\n",
    "# Choose a multiple of 32 (warp size) for best performance\n",
    "threads_per_block = 256\n",
    "\n",
    "# Calculate number of blocks needed:\n",
    "# - data.size is the total number of elements\n",
    "# - (data.size + threads_per_block - 1) ensures we round up the division\n",
    "blocks_per_grid = (data.size + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "print(f\"Grid configuration: {blocks_per_grid} blocks, {threads_per_block} threads per block\")\n",
    "\n",
    "# Transfer data to GPU\n",
    "# cuda.to_device allocates memory on GPU and copies data from CPU\n",
    "d_data = cuda.to_device(data)\n",
    "d_result = cuda.to_device(result_gpu)\n",
    "\n",
    "# Launch the kernel with our grid configuration\n",
    "# Syntax: kernel[blocks_per_grid, threads_per_block](arguments...)\n",
    "gpu_square_array[blocks_per_grid, threads_per_block](d_data, d_result)\n",
    "\n",
    "# Transfer results back to CPU with copy_to_host()\n",
    "result_gpu = d_result.copy_to_host()\n",
    "print(\"GPU Result:\", result_gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the GPU Implementation\n",
    "\n",
    "Let's break down the key components and concepts:\n",
    "\n",
    "1. **Kernel Decoration**\n",
    "   ```python\n",
    "   @cuda.jit\n",
    "   ```\n",
    "   - This decorator marks the function as a CUDA kernel\n",
    "   - It tells Numba to compile the function for GPU execution\n",
    "   - The function will be run in parallel by many GPU threads\n",
    "\n",
    "2. **Thread Indexing**\n",
    "   ```python\n",
    "   idx = cuda.grid(1)\n",
    "   ```\n",
    "   - Gets the absolute thread ID in a 1D grid\n",
    "   - Each thread needs to know which array element it's responsible for\n",
    "   - The '1' parameter indicates we're working with a 1D grid\n",
    "\n",
    "3. **Bounds Checking**\n",
    "   ```python\n",
    "   if idx < in_arr.size:\n",
    "   ```\n",
    "   - Critical for preventing out-of-bounds memory access\n",
    "   - Necessary because we might launch more threads than array elements\n",
    "   - GPU threads that exceed array bounds will do nothing\n",
    "\n",
    "4. **Grid Configuration**\n",
    "   ```python\n",
    "   threads_per_block = 256\n",
    "   blocks_per_grid = (data.size + threads_per_block - 1) // threads_per_block\n",
    "   ```\n",
    "   - threads_per_block: how many threads run in parallel within each block\n",
    "     - Usually a multiple of 32 (warp size)\n",
    "     - Maximum is typically 1024\n",
    "   - blocks_per_grid: how many blocks we need\n",
    "     - Formula ensures we have enough threads to cover all elements\n",
    "     - The division is rounded up to ensure coverage\n",
    "\n",
    "5. **Memory Transfers**\n",
    "   ```python\n",
    "   d_data = cuda.to_device(data)\n",
    "   d_result = cuda.to_device(result_gpu)\n",
    "   result_gpu = d_result.copy_to_host()\n",
    "   ```\n",
    "   - to_device(): Allocates GPU memory and copies data from CPU to GPU\n",
    "   - copy_to_host(): Copies results from GPU back to CPU\n",
    "   - 'd_' prefix is a common convention for GPU (device) variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Let's compare the performance of CPU and GPU versions with a larger array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU time: 0.1302 seconds\n",
      "GPU time: 0.0122 seconds (including data transfers)\n",
      "Speedup: 10.70x\n",
      "Results match: True\n"
     ]
    }
   ],
   "source": [
    "# Create a larger array for meaningful comparison\n",
    "large_data = np.random.random(1000000).astype(np.float64)\n",
    "\n",
    "# Time CPU version\n",
    "start_cpu = time.perf_counter()\n",
    "result_cpu = cpu_square_array(large_data)\n",
    "cpu_time = time.perf_counter() - start_cpu\n",
    "\n",
    "# Time GPU version (including data transfers)\n",
    "start_gpu = time.perf_counter()\n",
    "d_large_data = cuda.to_device(large_data)\n",
    "d_result = cuda.to_device(np.zeros_like(large_data))\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (large_data.size + threads_per_block - 1) // threads_per_block\n",
    "gpu_square_array[blocks_per_grid, threads_per_block](d_large_data, d_result)\n",
    "\n",
    "result_gpu = d_result.copy_to_host()\n",
    "gpu_time = time.perf_counter() - start_gpu\n",
    "\n",
    "print(f\"CPU time: {cpu_time:.4f} seconds\")\n",
    "print(f\"GPU time: {gpu_time:.4f} seconds (including data transfers)\")\n",
    "print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "\n",
    "# Verify results are the same\n",
    "print(\"Results match:\", np.allclose(result_cpu, result_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pattern 2: Device Functions** <a name=\"device-functions\"></a>\n",
    "\n",
    "Device functions are helper functions that run on the GPU. This pattern introduces several new concepts including:\n",
    "- Function composition on GPU\n",
    "- Multiple GPU functions working together\n",
    "- Device-specific function decoration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Device Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input points:\n",
      "Point 0: (0.0, 0.0)\n",
      "Point 1: (1.0, 1.0)\n",
      "Point 2: (2.0, 2.0)\n",
      "Point 3: (3.0, 3.0)\n",
      "\n",
      "Calculated distances between consecutive points:\n",
      "Distance between points 0 and 1: 1.4142\n",
      "Distance between points 1 and 2: 1.4142\n",
      "Distance between points 2 and 3: 1.4142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# The device=True parameter indicates this is a GPU helper function\n",
    "# Unlike regular CUDA kernels, device functions can return values\n",
    "# They can ONLY be called from other GPU functions, not from CPU code\n",
    "@cuda.jit(device=True)\n",
    "def device_distance(x1, y1, x2, y2):\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "# This is our main kernel that uses the device function\n",
    "@cuda.jit\n",
    "def gpu_calculate_distances(points_x, points_y, distances):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < points_x.size - 1:\n",
    "        # Device functions can be called just like regular functions\n",
    "        # inside GPU kernels\n",
    "        distances[idx] = device_distance(\n",
    "            points_x[idx], points_y[idx],\n",
    "            points_x[idx + 1], points_y[idx + 1]\n",
    "        )\n",
    "\n",
    "# Example usage with detailed prints for understanding\n",
    "points_x = np.array([0, 1, 2, 3], dtype=np.float64)\n",
    "points_y = np.array([0, 1, 2, 3], dtype=np.float64)\n",
    "distances = np.zeros(len(points_x) - 1, dtype=np.float64)\n",
    "\n",
    "print(\"Input points:\")\n",
    "for i in range(len(points_x)):\n",
    "    print(f\"Point {i}: ({points_x[i]}, {points_y[i]})\")\n",
    "\n",
    "# GPU setup and execution\n",
    "d_points_x = cuda.to_device(points_x)\n",
    "d_points_y = cuda.to_device(points_y)\n",
    "d_distances = cuda.to_device(distances)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (len(points_x) + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "gpu_calculate_distances[blocks_per_grid, threads_per_block](d_points_x, d_points_y, d_distances)\n",
    "distances = d_distances.copy_to_host()\n",
    "\n",
    "print(\"\\nCalculated distances between consecutive points:\")\n",
    "for i in range(len(distances)):\n",
    "    print(f\"Distance between points {i} and {i+1}: {distances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key New Concepts\n",
    "\n",
    "1. **Device Functions (`@cuda.jit(device=True)`)**\n",
    "   - Helper functions that run on the GPU\n",
    "   - Can return values (unlike kernel functions)\n",
    "   - Can only be called by other GPU functions\n",
    "   - Useful for code organization and reusability\n",
    "   - Don't require grid/block configuration as they're not entry points\n",
    "\n",
    "2. **Function Composition**\n",
    "   ```python\n",
    "   distances[idx] = device_distance(points_x[idx], points_y[idx],\n",
    "                                  points_x[idx + 1], points_y[idx + 1])\n",
    "   ```\n",
    "   - GPU functions can call other GPU functions\n",
    "   - Helps break down complex computations\n",
    "   - Improves code readability and maintainability\n",
    "\n",
    "3. **Data Access Patterns**\n",
    "   ```python\n",
    "   if idx < points_x.size - 1:\n",
    "   ```\n",
    "   - Note the `-1` in the bounds check\n",
    "   - When computing distances between consecutive points, we need N-1 distances for N points\n",
    "   - Shows how to handle computations that reference adjacent elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Device Functions vs Kernels\n",
    "\n",
    "| Aspect | Device Function | Kernel |\n",
    "|--------|----------------|---------|\n",
    "| Decoration | `@cuda.jit(device=True)` | `@cuda.jit` |\n",
    "| Can Return Values | Yes | No |\n",
    "| Called From | GPU functions only | CPU code |\n",
    "| Grid/Block Config | Not needed | Required |\n",
    "| Primary Use | Helper logic | Main entry point |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Multiple Device Functions\n",
    "\n",
    "Let's extend our example to show how multiple device functions can work together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extended Analysis Results:\n",
      "Segment 0:\n",
      "  Distance: 2.8284\n",
      "  Midpoint: (1.0, 1.0)\n",
      "Segment 1:\n",
      "  Distance: 2.8284\n",
      "  Midpoint: (3.0, 3.0)\n",
      "Segment 2:\n",
      "  Distance: 2.8284\n",
      "  Midpoint: (5.0, 5.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit(device=True)\n",
    "def device_distance(x1, y1, x2, y2):\n",
    "    return ((x1 - x2) ** 2 + (y1 - y2) ** 2) ** 0.5\n",
    "\n",
    "@cuda.jit(device=True)\n",
    "def device_midpoint(x1, y1, x2, y2):\n",
    "    return (x1 + x2) / 2, (y1 + y2) / 2\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_analyze_points(points_x, points_y, distances, midpoints_x, midpoints_y):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < points_x.size - 1:\n",
    "        # Calculate distance using first device function\n",
    "        distances[idx] = device_distance(\n",
    "            points_x[idx], points_y[idx],\n",
    "            points_x[idx + 1], points_y[idx + 1]\n",
    "        )\n",
    "        # Calculate midpoint using second device function\n",
    "        midpoints_x[idx], midpoints_y[idx] = device_midpoint(\n",
    "            points_x[idx], points_y[idx],\n",
    "            points_x[idx + 1], points_y[idx + 1]\n",
    "        )\n",
    "\n",
    "# Example usage\n",
    "points_x = np.array([0, 2, 4, 6], dtype=np.float64)\n",
    "points_y = np.array([0, 2, 4, 6], dtype=np.float64)\n",
    "distances = np.zeros(len(points_x) - 1, dtype=np.float64)\n",
    "midpoints_x = np.zeros(len(points_x) - 1, dtype=np.float64)\n",
    "midpoints_y = np.zeros(len(points_x) - 1, dtype=np.float64)\n",
    "\n",
    "# Transfer data and execute\n",
    "d_points_x = cuda.to_device(points_x)\n",
    "d_points_y = cuda.to_device(points_y)\n",
    "d_distances = cuda.to_device(distances)\n",
    "d_midpoints_x = cuda.to_device(midpoints_x)\n",
    "d_midpoints_y = cuda.to_device(midpoints_y)\n",
    "\n",
    "threads_per_block = 256\n",
    "blocks_per_grid = (len(points_x) + threads_per_block - 1) // threads_per_block\n",
    "\n",
    "gpu_analyze_points[blocks_per_grid, threads_per_block](\n",
    "    d_points_x, d_points_y, d_distances, d_midpoints_x, d_midpoints_y\n",
    ")\n",
    "\n",
    "# Get results\n",
    "distances = d_distances.copy_to_host()\n",
    "midpoints_x = d_midpoints_x.copy_to_host()\n",
    "midpoints_y = d_midpoints_y.copy_to_host()\n",
    "\n",
    "print(\"\\nExtended Analysis Results:\")\n",
    "for i in range(len(distances)):\n",
    "    print(f\"Segment {i}:\")\n",
    "    print(f\"  Distance: {distances[i]:.4f}\")\n",
    "    print(f\"  Midpoint: ({midpoints_x[i]:.1f}, {midpoints_y[i]:.1f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for Device Functions\n",
    "\n",
    "1. Keep device functions focused on a single task\n",
    "2. Use device functions to avoid code duplication in kernels\n",
    "3. Consider using device functions for complex calculations that are used repeatedly\n",
    "4. Remember that device functions add a small overhead compared to inlined code\n",
    "5. Use them to improve code readability and maintainability when the performance impact is acceptable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pattern 3: Memory Management** <a name=\"memory\"></a>\n",
    "\n",
    "Memory management on GPUs is crucial for performance. This pattern introduces:\n",
    "- Shared memory usage\n",
    "- Thread synchronization\n",
    "- Memory access patterns\n",
    "- Different types of GPU memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding GPU Memory Types\n",
    "\n",
    "1. **Global Memory**\n",
    "   - Accessible by all threads across all blocks\n",
    "   - Slowest type of memory\n",
    "   - Used for main data storage (what we used in Patterns 1 & 2)\n",
    "\n",
    "2. **Shared Memory**\n",
    "   - Shared between threads within the same block\n",
    "   - Much faster than global memory\n",
    "   - Limited size (typically 48KB per block)\n",
    "   - Must be explicitly managed\n",
    "\n",
    "3. **Local Memory**\n",
    "   - Private to each thread\n",
    "   - Automatically managed by the compiler\n",
    "   - Used for thread-local variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Using Shared Memory for Moving Average\n",
    "\n",
    "This example calculates a moving average of array elements using shared memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [1. 4. 6. 2. 8. 5. 9. 3.]\n",
      "\n",
      "Moving average results (first 8 elements): [1.         3.66666667 4.         5.33333333 5.         7.33333333\n",
      " 5.66666667 4.        ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def gpu_shared_memory_example(input_array, output_array):\n",
    "    # Declare shared memory array\n",
    "    # Size must be known at compile time\n",
    "    # Each thread block gets its own copy of shared memory\n",
    "    shared_array = cuda.shared.array(256, dtype=float64)\n",
    "    \n",
    "    # Get thread indices\n",
    "    idx = cuda.grid(1)  # Global thread ID\n",
    "    thread_id = cuda.threadIdx.x  # Local thread ID within the block\n",
    "    \n",
    "    # Phase 1: Load data into shared memory\n",
    "    if idx < input_array.size:\n",
    "        shared_array[thread_id] = input_array[idx]\n",
    "    else:\n",
    "        # Pad with zeros if we're beyond array bounds\n",
    "        shared_array[thread_id] = 0.0\n",
    "    \n",
    "    # CRITICAL: Synchronize threads before proceeding\n",
    "    # Ensures all threads have written to shared memory\n",
    "    cuda.syncthreads()\n",
    "    \n",
    "    # Phase 2: Process data using shared memory\n",
    "    if idx < input_array.size:\n",
    "        # Calculate average of current element with neighbors\n",
    "        if thread_id > 0 and thread_id < 255:\n",
    "            output_array[idx] = (shared_array[thread_id - 1] + \n",
    "                               shared_array[thread_id] + \n",
    "                               shared_array[thread_id + 1]) / 3\n",
    "        else:\n",
    "            # Handle edge cases (first and last elements)\n",
    "            output_array[idx] = shared_array[thread_id]\n",
    "\n",
    "# Create example data and show how the moving average works\n",
    "data = np.array([1, 4, 6, 2, 8, 5, 9, 3], dtype=np.float64)\n",
    "print(\"Original data:\", data)\n",
    "\n",
    "# Pad data to match shared memory size\n",
    "padded_data = np.zeros(256, dtype=np.float64)\n",
    "padded_data[:len(data)] = data\n",
    "result = np.zeros_like(padded_data)\n",
    "\n",
    "# Transfer data to GPU\n",
    "d_data = cuda.to_device(padded_data)\n",
    "d_result = cuda.to_device(result)\n",
    "\n",
    "# Launch kernel with exactly one block of 256 threads\n",
    "gpu_shared_memory_example[1, 256](d_data, d_result)\n",
    "\n",
    "# Get results\n",
    "result = d_result.copy_to_host()\n",
    "print(\"\\nMoving average results (first 8 elements):\", result[:8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Concepts Explained\n",
    "\n",
    "1. **Shared Memory Declaration**\n",
    "   ```python\n",
    "   shared_array = cuda.shared.array(256, dtype=float64)\n",
    "   ```\n",
    "   - Creates a shared memory array within each thread block\n",
    "   - Size must be a compile-time constant\n",
    "   - Each block gets its own independent copy\n",
    "   - Faster than global memory but limited in size\n",
    "\n",
    "2. **Thread Identification**\n",
    "   ```python\n",
    "   idx = cuda.grid(1)          # Global thread ID across all blocks\n",
    "   thread_id = cuda.threadIdx.x  # Local thread ID within current block\n",
    "   ```\n",
    "   - Need both global and local IDs for different purposes\n",
    "   - Global ID (idx) for accessing input/output arrays\n",
    "   - Local ID (thread_id) for accessing shared memory\n",
    "\n",
    "3. **Thread Synchronization**\n",
    "   ```python\n",
    "   cuda.syncthreads()\n",
    "   ```\n",
    "   - Forces all threads in a block to wait at this point\n",
    "   - Critical when using shared memory\n",
    "   - Ensures all threads have finished writing before any start reading\n",
    "   - Only synchronizes threads within the same block\n",
    "\n",
    "4. **Memory Access Pattern**\n",
    "   ```python\n",
    "   if thread_id > 0 and thread_id < 255:\n",
    "       output_array[idx] = (shared_array[thread_id - 1] + \n",
    "                          shared_array[thread_id] + \n",
    "                          shared_array[thread_id + 1]) / 3\n",
    "   ```\n",
    "   - Shows how threads can access neighbors' data\n",
    "   - Uses bounds checking to handle edge cases\n",
    "   - Demonstrates shared memory's advantage for repeated access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison\n",
    "\n",
    "Let's compare the performance of shared memory vs global memory approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shared Memory Time: 0.001889 seconds\n",
      "Global Memory Time: 0.050942 seconds\n",
      "Speedup: 26.96x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 40 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "# Global memory version (without shared memory)\n",
    "@cuda.jit\n",
    "def gpu_global_memory_example(input_array, output_array):\n",
    "    idx = cuda.grid(1)\n",
    "    if idx < input_array.size - 1 and idx > 0:\n",
    "        output_array[idx] = (input_array[idx - 1] + \n",
    "                           input_array[idx] + \n",
    "                           input_array[idx + 1]) / 3\n",
    "\n",
    "# Create larger test data\n",
    "large_data = np.random.random(10000).astype(np.float64)\n",
    "result_shared = np.zeros_like(large_data)\n",
    "result_global = np.zeros_like(large_data)\n",
    "\n",
    "# Test shared memory version\n",
    "start_shared = time.perf_counter()\n",
    "padded_data = np.zeros((((len(large_data) + 255) // 256) * 256), dtype=np.float64)\n",
    "padded_data[:len(large_data)] = large_data\n",
    "d_padded = cuda.to_device(padded_data)\n",
    "d_result = cuda.to_device(np.zeros_like(padded_data))\n",
    "\n",
    "blocks = (len(padded_data) + 255) // 256\n",
    "gpu_shared_memory_example[blocks, 256](d_padded, d_result)\n",
    "result_shared = d_result.copy_to_host()[:len(large_data)]\n",
    "shared_time = time.perf_counter() - start_shared\n",
    "\n",
    "# Test global memory version\n",
    "start_global = time.perf_counter()\n",
    "d_data = cuda.to_device(large_data)\n",
    "d_result = cuda.to_device(result_global)\n",
    "\n",
    "blocks = (len(large_data) + 255) // 256\n",
    "gpu_global_memory_example[blocks, 256](d_data, d_result)\n",
    "result_global = d_result.copy_to_host()\n",
    "global_time = time.perf_counter() - start_global\n",
    "\n",
    "print(f\"Shared Memory Time: {shared_time:.6f} seconds\")\n",
    "print(f\"Global Memory Time: {global_time:.6f} seconds\")\n",
    "print(f\"Speedup: {global_time/shared_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices for GPU Memory Management\n",
    "\n",
    "1. **Shared Memory Usage**\n",
    "   - Use shared memory when data will be accessed multiple times\n",
    "   - Be mindful of shared memory size limitations\n",
    "   - Consider padding to avoid bank conflicts\n",
    "\n",
    "2. **Thread Synchronization**\n",
    "   - Always synchronize after shared memory writes\n",
    "   - Minimize synchronization points\n",
    "   - Remember synchronization only works within a block\n",
    "\n",
    "3. **Memory Access Patterns**\n",
    "   - Strive for Aligned memory access\n",
    "   - Minimize divergent branching\n",
    "   - Consider padding to achieve better memory alignment\n",
    "\n",
    "4. **Data Transfer**\n",
    "   - Minimize host-device transfers\n",
    "   - Batch data transfers when possible\n",
    "   - Consider using pinned memory for faster transfers\n",
    "\n",
    "5. **Memory Allocation**\n",
    "   - Reuse allocated memory when possible\n",
    "   - Free memory when no longer needed\n",
    "   - Consider using memory pools for dynamic allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pattern 4: Kernel Launch Patterns** <a name=\"kernels\"></a>\n",
    "\n",
    "Understanding how to structure and launch kernels for multi-dimensional problems is crucial for GPU programming. This pattern introduces:\n",
    "- 2D grid and block configurations\n",
    "- Multi-dimensional thread indexing\n",
    "- Matrix operations on GPU\n",
    "- Advanced launch configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Transposition Example\n",
    "\n",
    "We'll implement matrix transposition as it's a perfect example for demonstrating 2D grid operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[[1. 2. 3.]\n",
      " [4. 5. 6.]]\n",
      "\n",
      "Matrix shape: (2, 3)\n",
      "\n",
      "Grid Configuration:\n",
      "Threads per block: (16, 16)\n",
      "Blocks per grid: (1, 1)\n",
      "\n",
      "Transposed matrix:\n",
      "[[1. 4.]\n",
      " [2. 5.]\n",
      " [3. 6.]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def gpu_matrix_transpose(input_matrix, output_matrix):\n",
    "    # Get 2D thread indices\n",
    "    row, col = cuda.grid(2)\n",
    "    \n",
    "    # Bounds checking is crucial for 2D operations\n",
    "    if row < input_matrix.shape[0] and col < input_matrix.shape[1]:\n",
    "        # Transpose operation: swap row and column indices\n",
    "        output_matrix[col, row] = input_matrix[row, col]\n",
    "\n",
    "# Create a sample matrix\n",
    "matrix = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6]\n",
    "], dtype=np.float64)\n",
    "\n",
    "print(\"Original matrix:\")\n",
    "print(matrix)\n",
    "print(\"\\nMatrix shape:\", matrix.shape)\n",
    "\n",
    "# Create output matrix with transposed shape\n",
    "result = np.zeros((matrix.shape[1], matrix.shape[0]), dtype=np.float64)\n",
    "\n",
    "# 2D grid and block configuration\n",
    "threads_per_block_2d = (16, 16)  # 16x16 = 256 threads per block\n",
    "blocks_per_grid_2d = (\n",
    "    (matrix.shape[0] + threads_per_block_2d[0] - 1) // threads_per_block_2d[0],\n",
    "    (matrix.shape[1] + threads_per_block_2d[1] - 1) // threads_per_block_2d[1]\n",
    ")\n",
    "\n",
    "print(\"\\nGrid Configuration:\")\n",
    "print(f\"Threads per block: {threads_per_block_2d}\")\n",
    "print(f\"Blocks per grid: {blocks_per_grid_2d}\")\n",
    "\n",
    "# Transfer data and execute\n",
    "d_matrix = cuda.to_device(matrix)\n",
    "d_result = cuda.to_device(result)\n",
    "\n",
    "gpu_matrix_transpose[blocks_per_grid_2d, threads_per_block_2d](d_matrix, d_result)\n",
    "\n",
    "result = d_result.copy_to_host()\n",
    "print(\"\\nTransposed matrix:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key New Concepts\n",
    "\n",
    "1. **2D Grid Configuration**\n",
    "   ```python\n",
    "   threads_per_block_2d = (16, 16)\n",
    "   blocks_per_grid_2d = (\n",
    "       (matrix.shape[0] + threads_per_block_2d[0] - 1) // threads_per_block_2d[0],\n",
    "       (matrix.shape[1] + threads_per_block_2d[1] - 1) // threads_per_block_2d[1]\n",
    "   )\n",
    "   ```\n",
    "   - Two-dimensional thread blocks (16Ã—16 = 256 threads per block)\n",
    "   - Grid dimensions calculated for each dimension separately\n",
    "   - Ceiling division ensures coverage of non-perfectly-divisible dimensions\n",
    "\n",
    "2. **2D Thread Indexing**\n",
    "   ```python\n",
    "   row, col = cuda.grid(2)\n",
    "   ```\n",
    "   - cuda.grid(2) returns a 2D tuple of indices\n",
    "   - Automatically calculates global position from block and thread indices\n",
    "   - Equivalent to manual calculation:\n",
    "     ```python\n",
    "     row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "     col = cuda.blockIdx.y * cuda.blockDim.y + cuda.threadIdx.y\n",
    "     ```\n",
    "\n",
    "3. **2D Bounds Checking**\n",
    "   ```python\n",
    "   if row < input_matrix.shape[0] and col < input_matrix.shape[1]:\n",
    "   ```\n",
    "   - Must check bounds for both dimensions\n",
    "   - Prevents out-of-bounds access in non-square matrices\n",
    "   - Handles cases where grid size exceeds matrix dimensions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended Example: Matrix Operations with Different Launch Patterns\n",
    "\n",
    "Let's explore three different ways to structure matrix transposition, each with its own advantages and trade-offs:\n",
    "1. 2D Grid (Original approach)\n",
    "2. Row-major approach (1D grid, each thread handles a row)\n",
    "3. Column-major approach (1D grid, each thread handles a column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix:\n",
      "[[ 0.  1.  2.  3.]\n",
      " [ 4.  5.  6.  7.]\n",
      " [ 8.  9. 10. 11.]\n",
      " [12. 13. 14. 15.]\n",
      " [16. 17. 18. 19.]\n",
      " [20. 21. 22. 23.]\n",
      " [24. 25. 26. 27.]\n",
      " [28. 29. 30. 31.]]\n",
      "\n",
      "Shape: (8, 4)\n",
      "\n",
      "1. Testing 2D Grid Version:\n",
      "Grid config: (1, 1) blocks, (16, 16) threads per block\n",
      "\n",
      "2. Testing Row-Major Version:\n",
      "Grid config: 1 blocks, 256 threads per block\n",
      "\n",
      "3. Testing Column-Major Version:\n",
      "Grid config: 1 blocks, 256 threads per block\n",
      "\n",
      "4. Testing Shared Memory Version:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n",
      "c:\\Users\\caleb\\anaconda3\\envs\\collisionEnv\\Lib\\site-packages\\numba\\cuda\\dispatcher.py:536: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Performance Comparison:\n",
      "2D Grid Version: 7.637 ms\n",
      "Row-Major Version: 57.077 ms\n",
      "Column-Major Version: 56.860 ms\n",
      "Shared Memory Version: 192.973 ms\n",
      "\n",
      "All results match: True\n"
     ]
    }
   ],
   "source": [
    "@cuda.jit\n",
    "def gpu_transpose_row_major(input_matrix, output_matrix):\n",
    "    # Single thread processes entire row\n",
    "    # blockIdx.x gives the block index\n",
    "    # blockDim.x gives the number of threads per block\n",
    "    # threadIdx.x gives the thread index within the block\n",
    "    row = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    if row < input_matrix.shape[0]:\n",
    "        # One thread processes entire row\n",
    "        # Advantage: Good memory coalescing for input matrix\n",
    "        # Disadvantage: Poor memory coalescing for output matrix\n",
    "        for col in range(input_matrix.shape[1]):\n",
    "            output_matrix[col, row] = input_matrix[row, col]\n",
    "\n",
    "# Version 2: Column-major processing\n",
    "@cuda.jit\n",
    "def gpu_transpose_col_major(input_matrix, output_matrix):\n",
    "    # Single thread processes entire column\n",
    "    col = cuda.blockIdx.x * cuda.blockDim.x + cuda.threadIdx.x\n",
    "    \n",
    "    if col < input_matrix.shape[1]:\n",
    "        # One thread processes entire column\n",
    "        # Advantage: Good memory coalescing for output matrix\n",
    "        # Disadvantage: Poor memory coalescing for input matrix\n",
    "        for row in range(input_matrix.shape[0]):\n",
    "            output_matrix[col, row] = input_matrix[row, col]\n",
    "\n",
    "# Version 3: 2D grid with shared memory (more advanced approach)\n",
    "@cuda.jit\n",
    "def gpu_transpose_shared_memory(input_matrix, output_matrix):\n",
    "    # Declare shared memory - size must be known at compile time\n",
    "    TILE_DIM = 16  # Should match block size for efficiency\n",
    "    tile = cuda.shared.array(shape=(TILE_DIM, TILE_DIM), dtype=float64)\n",
    "    \n",
    "    # Get 2D thread indices\n",
    "    row, col = cuda.grid(2)\n",
    "    block_row, block_col = cuda.threadIdx.x, cuda.threadIdx.y\n",
    "    \n",
    "    if row < input_matrix.shape[0] and col < input_matrix.shape[1]:\n",
    "        # Load data into shared memory\n",
    "        # This read is Aligned\n",
    "        tile[block_row, block_col] = input_matrix[row, col]\n",
    "        \n",
    "        # Wait for all threads to load their data\n",
    "        cuda.syncthreads()\n",
    "        \n",
    "        # Calculate output indices\n",
    "        out_row = col\n",
    "        out_col = row\n",
    "        \n",
    "        # Write to output matrix with transposed indices\n",
    "        # Shared memory helps make this write more efficient\n",
    "        if out_row < output_matrix.shape[0] and out_col < output_matrix.shape[1]:\n",
    "            output_matrix[out_row, out_col] = tile[block_row, block_col]\n",
    "\n",
    "# Create a larger test matrix for meaningful comparison\n",
    "test_matrix = np.arange(32, dtype=np.float64).reshape(8, 4)\n",
    "print(\"Original matrix:\")\n",
    "print(test_matrix)\n",
    "print(\"\\nShape:\", test_matrix.shape)\n",
    "\n",
    "# Prepare output arrays for each version\n",
    "result_2d = np.zeros((test_matrix.shape[1], test_matrix.shape[0]), dtype=np.float64)\n",
    "result_row = np.zeros_like(result_2d)\n",
    "result_col = np.zeros_like(result_2d)\n",
    "result_shared = np.zeros_like(result_2d)\n",
    "\n",
    "# Transfer input data to GPU\n",
    "d_test = cuda.to_device(test_matrix)\n",
    "\n",
    "# 1. Test 2D version (original)\n",
    "print(\"\\n1. Testing 2D Grid Version:\")\n",
    "threads_2d = (16, 16)\n",
    "blocks_2d = (\n",
    "    (test_matrix.shape[0] + threads_2d[0] - 1) // threads_2d[0],\n",
    "    (test_matrix.shape[1] + threads_2d[1] - 1) // threads_2d[1]\n",
    ")\n",
    "print(f\"Grid config: {blocks_2d} blocks, {threads_2d} threads per block\")\n",
    "\n",
    "d_result = cuda.to_device(result_2d)\n",
    "start = time.perf_counter()\n",
    "gpu_matrix_transpose[blocks_2d, threads_2d](d_test, d_result)\n",
    "cuda.synchronize()\n",
    "time_2d = time.perf_counter() - start\n",
    "result_2d = d_result.copy_to_host()\n",
    "\n",
    "# 2. Test Row-major version\n",
    "print(\"\\n2. Testing Row-Major Version:\")\n",
    "threads_1d = 256\n",
    "blocks_row = (test_matrix.shape[0] + threads_1d - 1) // threads_1d\n",
    "print(f\"Grid config: {blocks_row} blocks, {threads_1d} threads per block\")\n",
    "\n",
    "d_result = cuda.to_device(result_row)\n",
    "start = time.perf_counter()\n",
    "gpu_transpose_row_major[blocks_row, threads_1d](d_test, d_result)\n",
    "cuda.synchronize()\n",
    "time_row = time.perf_counter() - start\n",
    "result_row = d_result.copy_to_host()\n",
    "\n",
    "# 3. Test Column-major version\n",
    "print(\"\\n3. Testing Column-Major Version:\")\n",
    "blocks_col = (test_matrix.shape[1] + threads_1d - 1) // threads_1d\n",
    "print(f\"Grid config: {blocks_col} blocks, {threads_1d} threads per block\")\n",
    "\n",
    "d_result = cuda.to_device(result_col)\n",
    "start = time.perf_counter()\n",
    "gpu_transpose_col_major[blocks_col, threads_1d](d_test, d_result)\n",
    "cuda.synchronize()\n",
    "time_col = time.perf_counter() - start\n",
    "result_col = d_result.copy_to_host()\n",
    "\n",
    "# 4. Test Shared Memory version\n",
    "print(\"\\n4. Testing Shared Memory Version:\")\n",
    "d_result = cuda.to_device(result_shared)\n",
    "start = time.perf_counter()\n",
    "gpu_transpose_shared_memory[blocks_2d, threads_2d](d_test, d_result)\n",
    "cuda.synchronize()\n",
    "time_shared = time.perf_counter() - start\n",
    "result_shared = d_result.copy_to_host()\n",
    "\n",
    "# Compare results and timing\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(f\"2D Grid Version: {time_2d*1000:.3f} ms\")\n",
    "print(f\"Row-Major Version: {time_row*1000:.3f} ms\")\n",
    "print(f\"Column-Major Version: {time_col*1000:.3f} ms\")\n",
    "print(f\"Shared Memory Version: {time_shared*1000:.3f} ms\")\n",
    "\n",
    "print(\"\\nAll results match:\", \n",
    "      np.allclose(result_2d, result_row) and \n",
    "      np.allclose(result_2d, result_col) and\n",
    "      np.allclose(result_2d, result_shared))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Different Approaches\n",
    "\n",
    "1. **2D Grid Version**\n",
    "   - Uses a 2D grid of threads\n",
    "   - Each thread handles one element\n",
    "   - Advantages:\n",
    "     - Simple to understand\n",
    "     - Good parallelism\n",
    "   - Disadvantages:\n",
    "     - Memory access pattern not optimal\n",
    "     - More thread management overhead\n",
    "\n",
    "2. **Row-Major Version**\n",
    "   - Uses a 1D grid of threads\n",
    "   - Each thread handles an entire row\n",
    "   - Advantages:\n",
    "     - Aligned memory access for input matrix\n",
    "     - Simpler thread management\n",
    "   - Disadvantages:\n",
    "     - Less parallelism\n",
    "     - Poor memory access pattern for output matrix\n",
    "\n",
    "3. **Column-Major Version**\n",
    "   - Uses a 1D grid of threads\n",
    "   - Each thread handles an entire column\n",
    "   - Advantages:\n",
    "     - Aligned memory access for output matrix\n",
    "     - Simpler thread management\n",
    "   - Disadvantages:\n",
    "     - Less parallelism\n",
    "     - Poor memory access pattern for input matrix\n",
    "\n",
    "4. **Shared Memory Version**\n",
    "   - Uses 2D grid with shared memory tile\n",
    "   - Each thread block cooperatively loads and stores data\n",
    "   - Advantages:\n",
    "     - Better memory access patterns\n",
    "     - Can be much faster for larger matrices\n",
    "   - Disadvantages:\n",
    "     - More complex\n",
    "     - Limited by shared memory size\n",
    "     - Requires careful synchronization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways for Launch Patterns\n",
    "\n",
    "1. **Memory Access Patterns Matter**\n",
    "   - Coalesced memory access is crucial for performance\n",
    "   - Different approaches have different memory access characteristics\n",
    "   - Consider both read and write patterns\n",
    "\n",
    "2. **Thread Organization**\n",
    "   - 1D vs 2D grids affect parallelism and complexity\n",
    "   - Consider the problem structure when choosing\n",
    "   - Balance parallelism with overhead\n",
    "\n",
    "3. **Shared Memory Usage**\n",
    "   - Can dramatically improve performance\n",
    "   - Requires careful synchronization\n",
    "   - Limited resource - use wisely\n",
    "\n",
    "4. **Workload Distribution**\n",
    "   - Consider how work is divided among threads\n",
    "   - Balance thread utilization\n",
    "   - Watch for divergent execution paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Common Pitfalls <a name=\"best-practices\"></a>\n",
    "\n",
    "### 1. Memory Transfer Minimization\n",
    "- Minimize data transfers between CPU and GPU\n",
    "- Use asynchronous transfers when possible\n",
    "- Batch operations to reduce transfer overhead\n",
    "\n",
    "### 2. Thread Divergence\n",
    "- Avoid conditional statements that cause threads in a warp to take different paths\n",
    "- Structure conditions to affect entire warps when possible\n",
    "\n",
    "### 3. Memory Coalescing\n",
    "- Access memory in contiguous chunks\n",
    "- Align data structures to memory boundaries\n",
    "\n",
    "### 4. Occupancy\n",
    "- Balance thread block size with resource usage\n",
    "- Consider shared memory usage impact on occupancy\n",
    "\n",
    "### 5. Common Pitfalls\n",
    "- Not checking array bounds\n",
    "- Incorrect grid/block size calculations\n",
    "- Not synchronizing threads when necessary\n",
    "- Using too much shared memory\n",
    "\n",
    "## Performance Tips\n",
    "\n",
    "1. Profile your code using NVIDIA's profiling tools\n",
    "2. Experiment with different block sizes\n",
    "3. Use appropriate data types (float32 vs float64)\n",
    "4. Consider memory access patterns\n",
    "5. Balance parallelism with resource usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "1. Convert a vector dot product function to GPU code\n",
    "2. Implement a matrix multiplication kernel\n",
    "3. Create a parallel histogram calculation\n",
    "4. Implement a parallel prefix sum (scan) operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from numba import cuda, float32\n",
    "import math\n",
    "\n",
    "\"\"\"\n",
    "CUDA Conversion Practice Exercises\n",
    "--------------------------------\n",
    "Your task is to implement the body of each GPU kernel and its corresponding launch function.\n",
    "The CPU implementation and verification code is provided for each exercise.\n",
    "\"\"\"\n",
    "\n",
    "# Exercise 1: Vector Dot Product  (Level: Beginner)\n",
    "# Learning objectives:\n",
    "# - Basic CUDA kernel structure\n",
    "# - Simple parallel reduction\n",
    "# - Using shared memory for partial results\n",
    "# - Atomic operations for final sum\n",
    "\n",
    "def cpu_dot_product(a, b):\n",
    "    \"\"\"\n",
    "    Compute the dot product of two vectors.\n",
    "    \n",
    "    Parameters:\n",
    "        a, b: numpy arrays of same length\n",
    "    Returns:\n",
    "        float: dot product result\n",
    "    \"\"\"\n",
    "    return np.sum(a * b)\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_dot_product_kernel(a, b, result):\n",
    "    \"\"\"\n",
    "    CUDA kernel for vector dot product.\n",
    "    \n",
    "    Parameters:\n",
    "        a, b: device arrays of same length\n",
    "        result: device array of length 1 to store final dot product\n",
    "    \"\"\"\n",
    "    # TODO: Implement the kernel\n",
    "    # Hints:\n",
    "    # 1. Use shared memory for partial sums within each block\n",
    "    # 2. Each thread should process one or more pairs of elements\n",
    "    # 3. Use atomic add for the final reduction\n",
    "    pass\n",
    "\n",
    "def gpu_dot_product(a, b):\n",
    "    \"\"\"Launch function for dot product kernel\"\"\"\n",
    "    # TODO: Implement kernel launch configuration and data transfer\n",
    "    pass\n",
    "\n",
    "def verify_dot_product(n=1000000):\n",
    "    \"\"\"Verification function for your GPU implementation\"\"\"\n",
    "    # Create test data\n",
    "    a = np.random.random(n).astype(np.float32)\n",
    "    b = np.random.random(n).astype(np.float32)\n",
    "    \n",
    "    # Get CPU result for comparison\n",
    "    cpu_start = time.time()\n",
    "    cpu_result = cpu_dot_product(a, b)\n",
    "    cpu_time = time.time() - cpu_start\n",
    "    \n",
    "    # Get GPU result\n",
    "    gpu_start = time.time()\n",
    "    gpu_result = gpu_dot_product(a, b)\n",
    "    gpu_time = time.time() - gpu_start\n",
    "    \n",
    "    print(f\"Vector Dot Product Results (n={n:,}):\")\n",
    "    print(f\"CPU Result: {cpu_result:.6f}, Time: {cpu_time:.6f} seconds\")\n",
    "    print(f\"GPU Result: {gpu_result:.6f}, Time: {gpu_time:.6f} seconds\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    print(f\"Absolute difference: {abs(cpu_result - gpu_result):.10f}\\n\")\n",
    "\n",
    "# Exercise 2: Matrix Multiplication (Level: Intermediate)\n",
    "# Learning objectives:\n",
    "# - 2D grid and block structure\n",
    "# - Tiled matrix multiplication\n",
    "# - Shared memory for data reuse\n",
    "# - Memory coalescing considerations\n",
    "\n",
    "def cpu_matrix_multiply(a, b):\n",
    "    \"\"\"\n",
    "    Compute the product of two matrices.\n",
    "    \n",
    "    Parameters:\n",
    "        a: numpy 2D array of shape (M, K)\n",
    "        b: numpy 2D array of shape (K, N)\n",
    "    Returns:\n",
    "        numpy 2D array of shape (M, N): result of a @ b\n",
    "    \"\"\"\n",
    "    return np.dot(a, b)\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_matrix_multiply_kernel(a, b, c):\n",
    "    \"\"\"\n",
    "    CUDA kernel for matrix multiplication.\n",
    "    \n",
    "    Parameters:\n",
    "        a: device array of shape (M, K)\n",
    "        b: device array of shape (K, N)\n",
    "        c: device array of shape (M, N) for result\n",
    "    \"\"\"\n",
    "    # TODO: Implement the kernel\n",
    "    # Hints:\n",
    "    # 1. Use shared memory for tiles of input matrices\n",
    "    # 2. Each thread should compute one element of output\n",
    "    # 3. Ensure coalesced memory access\n",
    "    pass\n",
    "\n",
    "def gpu_matrix_multiply(a, b):\n",
    "    \"\"\"Launch function for matrix multiplication kernel\"\"\"\n",
    "    # TODO: Implement kernel launch configuration and data transfer\n",
    "    pass\n",
    "\n",
    "def verify_matrix_multiply(n=1000):\n",
    "    \"\"\"Verification function for your GPU implementation\"\"\"\n",
    "    # Create test matrices\n",
    "    a = np.random.random((n, n)).astype(np.float32)\n",
    "    b = np.random.random((n, n)).astype(np.float32)\n",
    "    \n",
    "    # Get CPU result\n",
    "    cpu_start = time.time()\n",
    "    cpu_result = cpu_matrix_multiply(a, b)\n",
    "    cpu_time = time.time() - cpu_start\n",
    "    \n",
    "    # Get GPU result\n",
    "    gpu_start = time.time()\n",
    "    gpu_result = gpu_matrix_multiply(a, b)\n",
    "    gpu_time = time.time() - gpu_start\n",
    "    \n",
    "    print(f\"Matrix Multiplication Results (n={n}x{n}):\")\n",
    "    print(f\"CPU Time: {cpu_time:.6f} seconds\")\n",
    "    print(f\"GPU Time: {gpu_time:.6f} seconds\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    print(f\"Max absolute difference: {np.max(np.abs(cpu_result - gpu_result)):.10f}\\n\")\n",
    "\n",
    "# Exercise 3: Histogram Calculation (Level: Intermediate-Advanced)\n",
    "# Learning objectives:\n",
    "# - Atomic operations for concurrent updates\n",
    "# - Handling race conditions\n",
    "# - Memory access patterns\n",
    "# - Load balancing\n",
    "\n",
    "def cpu_histogram(data, nbins, min_val, max_val):\n",
    "    \"\"\"\n",
    "    Compute histogram of input data.\n",
    "    \n",
    "    Parameters:\n",
    "        data: numpy array of values\n",
    "        nbins: number of histogram bins\n",
    "        min_val: minimum value for binning\n",
    "        max_val: maximum value for binning\n",
    "    Returns:\n",
    "        numpy array: histogram counts\n",
    "    \"\"\"\n",
    "    return np.histogram(data, bins=nbins, range=(min_val, max_val))[0]\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_histogram_kernel(data, histogram, min_val, max_val):\n",
    "    \"\"\"\n",
    "    CUDA kernel for histogram calculation.\n",
    "    \n",
    "    Parameters:\n",
    "        data: input device array\n",
    "        histogram: device array for histogram bins\n",
    "        min_val: minimum value for binning\n",
    "        max_val: maximum value for binning\n",
    "    \"\"\"\n",
    "    # TODO: Implement the kernel\n",
    "    # Hints:\n",
    "    # 1. Calculate bin index for each element\n",
    "    # 2. Use atomic operations to update bins\n",
    "    # 3. Handle out-of-range values\n",
    "    pass\n",
    "\n",
    "def gpu_histogram(data, nbins, min_val, max_val):\n",
    "    \"\"\"Launch function for histogram kernel\"\"\"\n",
    "    # TODO: Implement kernel launch configuration and data transfer\n",
    "    pass\n",
    "\n",
    "def verify_histogram(n=10000000, nbins=128):\n",
    "    \"\"\"Verification function for your GPU implementation\"\"\"\n",
    "    # Create test data\n",
    "    data = np.random.random(n).astype(np.float32)\n",
    "    min_val, max_val = 0, 1\n",
    "    \n",
    "    # Get CPU result\n",
    "    cpu_start = time.time()\n",
    "    cpu_result = cpu_histogram(data, nbins, min_val, max_val)\n",
    "    cpu_time = time.time() - cpu_start\n",
    "    \n",
    "    # Get GPU result\n",
    "    gpu_start = time.time()\n",
    "    gpu_result = gpu_histogram(data, nbins, min_val, max_val)\n",
    "    gpu_time = time.time() - gpu_start\n",
    "    \n",
    "    print(f\"Histogram Calculation Results (n={n:,}, bins={nbins}):\")\n",
    "    print(f\"CPU Time: {cpu_time:.6f} seconds\")\n",
    "    print(f\"GPU Time: {gpu_time:.6f} seconds\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    print(f\"Max absolute difference: {np.max(np.abs(cpu_result - gpu_result))}\\n\")\n",
    "\n",
    "# Exercise 4: Parallel Prefix Sum (Scan) (Level: Advanced)\n",
    "# Learning objectives:\n",
    "# - Complex shared memory patterns\n",
    "# - Multi-phase algorithms\n",
    "# - Block synchronization\n",
    "# - Work-efficient parallel algorithms\n",
    "\n",
    "def cpu_prefix_sum(data):\n",
    "    \"\"\"\n",
    "    Compute inclusive prefix sum (cumulative sum) of input array.\n",
    "    \n",
    "    Parameters:\n",
    "        data: numpy array\n",
    "    Returns:\n",
    "        numpy array: cumulative sum\n",
    "    \"\"\"\n",
    "    return np.cumsum(data)\n",
    "\n",
    "@cuda.jit\n",
    "def gpu_prefix_sum_kernel(data, output):\n",
    "    \"\"\"\n",
    "    CUDA kernel for prefix sum calculation.\n",
    "    \n",
    "    Parameters:\n",
    "        data: input device array\n",
    "        output: device array for results\n",
    "    \"\"\"\n",
    "    # TODO: Implement the kernel\n",
    "    # Hints:\n",
    "    # 1. Implement Blelloch scan algorithm\n",
    "    # 2. Use shared memory for block-level scan\n",
    "    # 3. Handle boundary conditions\n",
    "    pass\n",
    "\n",
    "def gpu_prefix_sum(data):\n",
    "    \"\"\"Launch function for prefix sum kernel\"\"\"\n",
    "    # TODO: Implement kernel launch configuration and data transfer\n",
    "    pass\n",
    "\n",
    "def verify_prefix_sum(n=1000000):\n",
    "    \"\"\"Verification function for your GPU implementation\"\"\"\n",
    "    # Create test data\n",
    "    data = np.random.random(n).astype(np.float32)\n",
    "    \n",
    "    # Get CPU result\n",
    "    cpu_start = time.time()\n",
    "    cpu_result = cpu_prefix_sum(data)\n",
    "    cpu_time = time.time() - cpu_start\n",
    "    \n",
    "    # Get GPU result\n",
    "    gpu_start = time.time()\n",
    "    gpu_result = gpu_prefix_sum(data)\n",
    "    gpu_time = time.time() - gpu_start\n",
    "    \n",
    "    print(f\"Prefix Sum Results (n={n:,}):\")\n",
    "    print(f\"CPU Time: {cpu_time:.6f} seconds\")\n",
    "    print(f\"GPU Time: {gpu_time:.6f} seconds\")\n",
    "    if gpu_time > 0:\n",
    "        print(f\"Speedup: {cpu_time/gpu_time:.2f}x\")\n",
    "    print(f\"Max absolute difference: {np.max(np.abs(cpu_result - gpu_result)):.10f}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\nExercise 1: Vector Dot Product\")\n",
    "    print(\"------------------------------\")\n",
    "    verify_dot_product()\n",
    "    \n",
    "    print(\"\\nExercise 2: Matrix Multiplication\")\n",
    "    print(\"--------------------------------\")\n",
    "    verify_matrix_multiply()\n",
    "    \n",
    "    print(\"\\nExercise 3: Histogram Calculation\")\n",
    "    print(\"--------------------------------\")\n",
    "    verify_histogram()\n",
    "    \n",
    "    print(\"\\nExercise 4: Parallel Prefix Sum\")\n",
    "    print(\"------------------------------\")\n",
    "    verify_prefix_sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ Congratulations!\n",
    "\n",
    "You've completed the CUDA programming exercises! By working through these examples, you've gained hands-on experience with fundamental GPU programming concepts:\n",
    "\n",
    "## ðŸŽ¯ Key Concepts Covered\n",
    "- Basic CUDA kernel structure and memory management  \n",
    "- Parallel reduction techniques\n",
    "- Shared memory optimization \n",
    "- Memory coalescing and access patterns\n",
    "- Atomic operations\n",
    "- Block and grid dimensioning\n",
    "- Complex parallel algorithms\n",
    "\n",
    "## ðŸ“š Next Steps\n",
    "To continue your GPU programming journey, consider:\n",
    "1. Experimenting with different block and grid sizes to optimize performance\n",
    "2. Adding error handling to your implementations \n",
    "3. Trying the same algorithms with larger datasets\n",
    "4. Implementing other parallel algorithms like:\n",
    "  - Sorting algorithms\n",
    "  - Graph algorithms\n",
    "  - Image processing kernels\n",
    "  - Stencil computations\n",
    "\n",
    "## ðŸ”§ Performance Tips\n",
    "Remember these key optimization strategies:\n",
    "- Minimize memory transfers between CPU and GPU\n",
    "- Use shared memory when possible\n",
    "- Ensure coalesced memory access\n",
    "- Balance occupancy and resource usage \n",
    "- Profile your code to identify bottlenecks\n",
    "\n",
    "## ðŸ“– Useful Resources\n",
    "- [CUDA Documentation](https://docs.nvidia.com/cuda/)\n",
    "- [Numba Documentation](https://numba.pydata.org/)\n",
    "- [CUDA C++ Programming Guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/)\n",
    "- [CUDA Blog](https://developer.nvidia.com/blog/tag/cuda/)\n",
    "\n",
    "## ðŸ’¡ Pro Tips\n",
    "- Always profile before optimizing\n",
    "- Start with simple implementations and optimize incrementally\n",
    "- Use power-of-2 sizes for arrays when possible\n",
    "- Consider memory alignment and access patterns\n",
    "- Test with various input sizes\n",
    "\n",
    "## ðŸž Debugging Tips\n",
    "- Use `cuda.syncthreads()` judiciously\n",
    "- Print intermediate results for small inputs\n",
    "- Check array bounds carefully\n",
    "- Verify results against CPU implementation\n",
    "- Use CUDA-GDB for complex issues\n",
    "\n",
    "Thanks for completing this tutorial! If you have questions or feedback, please fill out this [form](https://forms.gle/CZYBL9zVfPTixHFW6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collisionEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
